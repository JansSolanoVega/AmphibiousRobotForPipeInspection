{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import io\n",
    "import imageio\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovingMNIST = np.load('mnist_test_seq.npy').transpose(1, 0, 2, 3)#convert to numpy array\n",
    "np.random.shuffle(MovingMNIST)# Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test, Validation splits\n",
    "train_data = MovingMNIST[:800]         \n",
    "val_data = MovingMNIST[800:900]       \n",
    "test_data = MovingMNIST[900:1000]    \n",
    "\n",
    "def collate(batch):\n",
    "\n",
    "    # Add channel dim, scale pixels between 0 and 1, send to GPU\n",
    "    batch = torch.tensor(batch).unsqueeze(1)     \n",
    "    batch = batch / 255.0                        \n",
    "    batch = batch.to(device)                     \n",
    "\n",
    "    # Randomly pick 10 frames as input, 11th frame is target\n",
    "    rand = np.random.randint(10,20)                     \n",
    "    return batch[:,:,rand-10:rand], batch[:,:,rand]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n",
      "torch.Size([8, 1, 10, 64, 64]) , torch.Size([8, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Training Data Loader\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_data, shuffle=True, \n",
    "                        batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "\n",
    "# Validation Data Loader\n",
    "val_loader = DataLoader(val_data, shuffle=True, \n",
    "                        batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "\n",
    "# Train_loader es un arreglo de 100 batches donde cada batch tiene tamaño de 8(batch size)\n",
    "# Cada batch de un loader es una tupla, donde un elemento tiene una dimension de 8,1,10,64,64 y otro de 8,1,64,64\n",
    "for i in train_loader:\n",
    "    print(i[0].shape, \",\", i[1].shape)\n",
    "#for i in val_loader:\n",
    "#    print(i[0].shape, \",\", i[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 10, 64, 64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482020a9b4f049dc9d07f0acfaf191a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xf8\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e5a3e993a5412cbca822c62ded1915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xfb\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f751dd2ea34a4b289d45884fe491c4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xfb\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d709e2f3bc456fa974ba63d6a316da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfa\\xfa\\xfa\\xf9\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "input, _ = next(iter(val_loader))\n",
    "input = input.cpu().numpy() * 255.0#reconvert to numpy array \n",
    "print(input.shape)\n",
    "for (i, video) in enumerate(input.squeeze(1)[:4]):# Shape of (16,1,10,64,64)->after squeeze (16,10,64,64)->after selection (4,10,64,64)\n",
    "    # for frame in video:\n",
    "    #     cv2.namedWindow(\"Video\"+str(i), cv2.WINDOW_NORMAL)\n",
    "    #     cv2.imshow(\"Video\"+str(i), frame)\n",
    "    #     cv2.waitKey(200)\n",
    "    # cv2.destroyAllWindows() # close the window\n",
    "        \n",
    "    with io.BytesIO() as gif:\n",
    "       imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n",
    "       display(HBox([widgets.Image(value=gif.getvalue())]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from ConvLSTM import ConvLSTM\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, num_kernels, kernel_size, padding, \n",
    "                activation, frame_size, num_layers):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.sequential = nn.Sequential()\n",
    "\n",
    "        # Add First layer (Different in_channels than the rest)\n",
    "        self.sequential.add_module(\n",
    "            \"convlstm1\", ConvLSTM(\n",
    "                in_channels=num_channels, out_channels=num_kernels,\n",
    "                kernel_size=kernel_size, padding=padding, \n",
    "                activation=activation, frame_size=frame_size)\n",
    "        )\n",
    "\n",
    "        self.sequential.add_module(\n",
    "            \"batchnorm1\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "        ) \n",
    "\n",
    "        # Add rest of the layers\n",
    "        for l in range(2, num_layers+1):\n",
    "\n",
    "            self.sequential.add_module(\n",
    "                f\"convlstm{l}\", ConvLSTM(\n",
    "                    in_channels=num_kernels, out_channels=num_kernels,\n",
    "                    kernel_size=kernel_size, padding=padding, \n",
    "                    activation=activation, frame_size=frame_size)\n",
    "                )\n",
    "                \n",
    "            self.sequential.add_module(\n",
    "                f\"batchnorm{l}\", nn.BatchNorm3d(num_features=num_kernels)\n",
    "                ) \n",
    "\n",
    "        # Add Convolutional Layer to predict output frame\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=num_kernels, out_channels=num_channels,\n",
    "            kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Forward propagation through all the layers\n",
    "        output = self.sequential(X)\n",
    "\n",
    "        # Return only the last output frame\n",
    "        output = self.conv(output[:,:,-1])\n",
    "        \n",
    "        return nn.Sigmoid()(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input video frames are grayscale, thus single channel\n",
    "model = Seq2Seq(num_channels=1, num_kernels=64, \n",
    "kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \n",
    "frame_size=(64, 64), num_layers=3).to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Binary Cross Entropy, target pixel values either 0 or 1\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Training Loss:552.13 Validation Loss:439.25\n",
      "\n",
      "Epoch:2 Training Loss:346.84 Validation Loss:320.41\n",
      "\n",
      "Epoch:3 Training Loss:324.03 Validation Loss:309.77\n",
      "\n",
      "Epoch:4 Training Loss:315.89 Validation Loss:309.21\n",
      "\n",
      "Epoch:5 Training Loss:309.74 Validation Loss:300.58\n",
      "\n",
      "Epoch:6 Training Loss:307.77 Validation Loss:298.36\n",
      "\n",
      "Epoch:7 Training Loss:305.17 Validation Loss:309.01\n",
      "\n",
      "Epoch:8 Training Loss:303.62 Validation Loss:304.20\n",
      "\n",
      "Epoch:9 Training Loss:305.08 Validation Loss:295.47\n",
      "\n",
      "Epoch:10 Training Loss:295.23 Validation Loss:305.70\n",
      "\n",
      "Epoch:11 Training Loss:295.99 Validation Loss:289.10\n",
      "\n",
      "Epoch:12 Training Loss:292.61 Validation Loss:310.06\n",
      "\n",
      "Epoch:13 Training Loss:293.83 Validation Loss:292.67\n",
      "\n",
      "Epoch:14 Training Loss:289.98 Validation Loss:290.01\n",
      "\n",
      "Epoch:15 Training Loss:294.45 Validation Loss:278.42\n",
      "\n",
      "Epoch:16 Training Loss:283.89 Validation Loss:281.65\n",
      "\n",
      "Epoch:17 Training Loss:285.84 Validation Loss:274.57\n",
      "\n",
      "Epoch:18 Training Loss:284.70 Validation Loss:290.15\n",
      "\n",
      "Epoch:19 Training Loss:283.20 Validation Loss:273.15\n",
      "\n",
      "Epoch:20 Training Loss:284.09 Validation Loss:305.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = 0                                                 \n",
    "    model.train()                                                  \n",
    "    for batch_num, (input, target) in enumerate(train_loader, 1):  \n",
    "        output = model(input)                                     \n",
    "        loss = criterion(output.flatten(), target.flatten())       \n",
    "        loss.backward()                                            \n",
    "        optim.step()                                               \n",
    "        optim.zero_grad()                                           \n",
    "        train_loss += loss.item()                                 \n",
    "    train_loss /= len(train_loader.dataset)                       \n",
    "\n",
    "    val_loss = 0                                                 \n",
    "    model.eval()                                                   \n",
    "    with torch.no_grad():                                          \n",
    "        for input, target in val_loader:                          \n",
    "            output = model(input)                                   \n",
    "            loss = criterion(output.flatten(), target.flatten())   \n",
    "            val_loss += loss.item()                                \n",
    "    val_loss /= len(val_loader.dataset)                            \n",
    "\n",
    "    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n\".format(\n",
    "        epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test(batch):\n",
    "\n",
    "    # Last 10 frames are target\n",
    "    target = np.array(batch)[:,10:]                     \n",
    "    \n",
    "    # Add channel dim, scale pixels between 0 and 1, send to GPU\n",
    "    batch = torch.tensor(batch).unsqueeze(1)          \n",
    "    batch = batch / 255.0                             \n",
    "    batch = batch.to(device)                          \n",
    "    return batch, target\n",
    "\n",
    "# Test Data Loader\n",
    "test_loader = DataLoader(test_data,shuffle=True, \n",
    "                         batch_size=3, collate_fn=collate_test)\n",
    "\n",
    "# Get a batch\n",
    "batch, target = next(iter(test_loader))\n",
    "\n",
    "# Initialize output sequence\n",
    "output = np.zeros(target.shape, dtype=np.uint8)\n",
    "\n",
    "# Loop over timesteps\n",
    "for timestep in range(target.shape[1]):\n",
    "  input = batch[:,:,timestep:timestep+10]   \n",
    "  output[:,timestep]=(model(input).squeeze(1).cpu()>0.5)*255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9b9c7e894141b2a4bc98144b67f397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xfb\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3458cca19bc243f78088e71af0c61408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xf9\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e132d059054b37b7200c9c012c58b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x87\\x00\\x00\\xff\\xff\\xff\\xfe\\xfe\\xfe\\xfd\\xfd\\xfd\\xfc\\xfc\\xfc\\xfb\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tgt, out in zip(target, output):       # Loop over samples\n",
    "    \n",
    "    # Write target video as gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, tgt, \"GIF\", fps = 5)    \n",
    "        target_gif = gif.getvalue()\n",
    "\n",
    "    # Write output video as gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, out, \"GIF\", fps = 5)    \n",
    "        output_gif = gif.getvalue()\n",
    "\n",
    "    display(HBox([widgets.Image(value=target_gif), \n",
    "                  widgets.Image(value=output_gif)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
